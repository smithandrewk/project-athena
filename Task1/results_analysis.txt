By referencing the above results, it can be seen that the Athena Ensemble framework was able to successfuly defend a Convolutional Neural Network from adversarial examples with small epsilon values. An effective adversarial attack may be able to cause small perturbations to input data, even imperceptible to the human eye, in order to decrease a deep neural network's ability to classify input data accurately. Therefore, it is much more important for an adversarial defense strategy to be able to defend against these examples. In production, one may be able to use an adversarial classifier to filter out input data that deviates too much from expected input data. As the changes to this data becomes more imperceptible, it would be much more difficult for such a classifier to identify these adversarial examples. This creates a flaw in the adversarial robustness of the model. Through evaluating these examples with the Athena framework, it shows the ability to decrease the error rate of performing inference on input data with small changes, thus increasing the robustness. By comparing to the PGD-ADT model, it can be seen that the Athena framework not only defended against these samples accurately, but up to par with the state of the art. 
