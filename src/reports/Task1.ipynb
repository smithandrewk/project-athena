{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contribution\n",
    "Andrew - Generated fork repository, contributed in programming process, ran programs locally, co-authored task 1 report\n",
    "\n",
    "JJ - Contributed in programming process, co-authored task 1 report\n",
    "\n",
    "Stephen - Contributed in programming process, co-authored task 1 report\n",
    "\n",
    "# Implementation\n",
    "In this report, we successfully implemented the attack of three different models; an Undefended Model, a Vanilla Athena Ensemble Model, and a PGD-ADT model. We chose to craft adversarial examples using the Fast Gradient Sign Method (FGSM), Jacobian-based Saliency Map Attack (JSMA), and Projected Gradient Descent (PGD).\n",
    "\n",
    "### FGSM\n",
    "The Fast Gradient Sign Method generates an adversarial example which maximizes loss by using gradients of the loss function with respect to the input image. With this method, we find out how much each input pixel contributes to the loss and add perturbations in order to maximize loss in the adversarial example.\n",
    "\n",
    "### BIM\n",
    "The Basic Iterative Method (BIM), also known as the Iterative FGSM (I-FGSM) is an extension of FGSM in which the attack performs a simple FGSM attack multiple times with smaller step sizes. \n",
    "\n",
    "### PGD\n",
    "The Projected Gradient Descent is a type of attack where the attacker has knowledge of the modelâ€™s gradients/weights. (A matrix that corresponds to how the model weighs each particular feature it detects) This threat model focuses on finding the perturbation that maximises the error of particular feature gradient of an input without crossing some threshold labelled as epsilon. The goal here is to find the minimum gradient of features that an input must contain to be classified incorrectly. \n",
    "\n",
    "# Experimentation\n",
    "\n",
    "For task one, our group decided to plot the following attacks:\n",
    "     - FGSM\n",
    "         Epsilons: 0.1, 0.5, 0.9\n",
    "     - BIM\n",
    "         Epsilons: 0.1, 0.5, 0.9\n",
    "     - PGD\n",
    "         Epsilons: 0.1, 0.5, 0.9\n",
    "         \n",
    "We used the following defense models:\n",
    "    - Undefended Model\n",
    "        No defense configuration.\n",
    "    - Vanilla Athena Average Probability Model\n",
    "        19 weak defenses consisting of a clean model followed by 18\n",
    "        models trained on transformed input data. The full \n",
    "        configuration of this model can be located at\n",
    "        \"configs/task1/athena-mnist.json\".\n",
    "    - PGD Model\n",
    "        A provided baseline model.\n",
    "        \n",
    "### Results\n",
    "RESULTS WILL GO HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Adversarial Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-a7c04e6d7113>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_lenet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_from_json\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0merror_rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from utils.model import load_lenet\n",
    "from utils.file import load_from_json\n",
    "from utils.metrics import error_rate\n",
    "from attacks.attack import generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ae(model, data, labels, attack_configs, save=False, output_dir=None):\n",
    "    \"\"\"\n",
    "    Generate adversarial examples\n",
    "    :param model: WeakDefense. The targeted model.\n",
    "    :param data: array. The benign samples to generate adversarial for.\n",
    "    :param labels: array or list. The true labels.\n",
    "    :param attack_configs: dictionary. Attacks and corresponding settings.\n",
    "    :param save: boolean. True, if save the adversarial examples.\n",
    "    :param output_dir: str or path. Location to save the adversarial examples.\n",
    "        It cannot be None when save is True.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    img_rows, img_cols = data.shape[1], data.shape[2]\n",
    "    num_attacks = attack_configs.get(\"num_attacks\")\n",
    "    data_loader = (data, labels)\n",
    "\n",
    "    if len(labels.shape) > 1:\n",
    "        labels = np.asarray([np.argmax(p) for p in labels])\n",
    "\n",
    "    # generate attacks one by one\n",
    "    for id in range(num_attacks):\n",
    "        key = \"configs{}\".format(id)\n",
    "        data_adv = generate(model=model,\n",
    "                            data_loader=data_loader,\n",
    "                            attack_args=attack_configs.get(key)\n",
    "                            )\n",
    "        print(attack_configs.get(key))\n",
    "        # predict the adversarial examples\n",
    "        predictions = model.predict(data_adv)\n",
    "        predictions = np.asarray([np.argmax(p) for p in predictions])\n",
    "\n",
    "        err = error_rate(y_pred=predictions, y_true=labels)\n",
    "        print(\">>> error rate:\", err)\n",
    "\n",
    "        # plotting some examples\n",
    "        num_plotting = min(data.shape[0], 0)\n",
    "        for i in range(10):\n",
    "            img = data_adv[i].reshape((img_rows, img_cols))\n",
    "            plt.imshow(img, cmap='gray')\n",
    "            title = '{}: {}->{}'.format(attack_configs.get(key).get(\"description\"),\n",
    "                                        labels[i],\n",
    "                                        predictions[i]\n",
    "                                        )\n",
    "            desc = str(attack_configs.get(key).get(\"description\"))\n",
    "            initial_label = str(labels[i])#AS\n",
    "            predicted_label = str(predictions[i])#AS\n",
    "            plt.title(title)\n",
    "            # Save plot\n",
    "            plt.savefig(\"../../results/figures/\"+desc+\"/\"+initial_label+\"->\"+predicted_label+\".jpg\")\n",
    "            plt.close()\n",
    "\n",
    "        # save the adversarial example\n",
    "        if save:\n",
    "            if output_dir is None:\n",
    "                raise ValueError(\"Cannot save images to a none path.\")\n",
    "            # save with a random name\n",
    "            file = os.path.join(output_dir, \"{}.npy\".format(desc))\n",
    "            print(\"Save the adversarial examples to file [{}].\".format(file))\n",
    "            np.save(file, data_adv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_from_json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-c5272066fa89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# parse configurations (into a dictionary) from json file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel_configs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_from_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../configs/task1/model-mnist.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdata_configs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_from_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../configs/task1/data-mnist.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mattack_configs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_from_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../configs/task1/attack-zk-mnist.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_from_json' is not defined"
     ]
    }
   ],
   "source": [
    "# parse configurations (into a dictionary) from json file\n",
    "model_configs = load_from_json('../configs/task1/model-mnist.json')\n",
    "data_configs = load_from_json('../configs/task1/data-mnist.json')\n",
    "attack_configs = load_from_json('../configs/task1/attack-zk-mnist.json')\n",
    "\n",
    "# load the targeted model\n",
    "model_file = os.path.join(model_configs.get(\"dir\"), model_configs.get(\"um_file\"))\n",
    "target = load_lenet(file=model_file, wrap=True)\n",
    "\n",
    "# load the benign samples\n",
    "data_file = os.path.join(data_configs.get('dir'), data_configs.get('bs_file'))\n",
    "data_bs = np.load(data_file)\n",
    "\n",
    "# load the corresponding true labels\n",
    "label_file = os.path.join(data_configs.get('dir'), data_configs.get('label_file'))\n",
    "labels = np.load(label_file)\n",
    "\n",
    "# generate adversarial examples for a small subset\n",
    "data_bs = data_bs[:10]\n",
    "labels = labels[:10]\n",
    "generate_ae(model=target, data=data_bs, labels=labels, attack_configs=attack_configs,save=False,output_dir=data_configs.get(\"dir\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Adversarial Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(trans_configs, model_configs,\n",
    "             data_configs, save=False, output_dir=None):\n",
    "    \"\"\"\n",
    "    Apply transformation(s) on images.\n",
    "    :param trans_configs: dictionary. The collection of the parameterized transformations to test.\n",
    "        in the form of\n",
    "        { configsx: {\n",
    "            param: value,\n",
    "            }\n",
    "        }\n",
    "        The key of a configuration is 'configs'x, where 'x' is the id of corresponding weak defense.\n",
    "    :param model_configs:  dictionary. Defines model related information.\n",
    "        Such as, location, the undefended model, the file format, etc.\n",
    "    :param data_configs: dictionary. Defines data related information.\n",
    "        Such as, location, the file for the true labels, the file for the benign samples,\n",
    "        the files for the adversarial examples, etc.\n",
    "    :param save: boolean. Save the transformed sample or not.\n",
    "    :param output_dir: path or str. The location to store the transformed samples.\n",
    "        It cannot be None when save is True.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # Load the baseline defense (PGD-ADT model)\n",
    "    baseline = load_lenet(file=model_configs.get('pgd_trained'), trans_configs=None,\n",
    "                                  use_logits=False, wrap=False)\n",
    "\n",
    "    # get the undefended model (UM)\n",
    "    file = os.path.join(model_configs.get('dir'), model_configs.get('um_file'))\n",
    "    undefended = load_lenet(file=file,\n",
    "                            trans_configs=trans_configs.get('configs0'),\n",
    "                            wrap=True)\n",
    "    print(\">>> um:\", type(undefended))\n",
    "\n",
    "    # load weak defenses into a pool\n",
    "    pool, _ = load_pool(trans_configs=trans_configs,\n",
    "                        model_configs=model_configs,\n",
    "                        active_list=True,\n",
    "                        wrap=True)\n",
    "    # create an AVEP ensemble from the WD pool\n",
    "    wds = list(pool.values())\n",
    "    print(\">>> wds:\", type(wds), type(wds[0]))\n",
    "    ensemble = Ensemble(classifiers=wds, strategy=ENSEMBLE_STRATEGY.AVEP.value)\n",
    "\n",
    "    # load the benign samples\n",
    "    bs_file = os.path.join(data_configs.get('dir'), data_configs.get('bs_file'))\n",
    "    x_bs = np.load(bs_file)\n",
    "    img_rows, img_cols = x_bs.shape[1], x_bs.shape[2]\n",
    "\n",
    "    # load the corresponding true labels\n",
    "    label_file = os.path.join(data_configs.get('dir'), data_configs.get('label_file'))\n",
    "    labels = np.load(label_file)\n",
    "\n",
    "    # get indices of benign samples that are correctly classified by the targeted model\n",
    "    print(\">>> Evaluating UM on [{}], it may take a while...\".format(bs_file))\n",
    "    pred_bs = undefended.predict(x_bs)\n",
    "    corrections = get_corrections(y_pred=pred_bs, y_true=labels)\n",
    "\n",
    "    # Evaluate AEs.\n",
    "    results = {}\n",
    "    ae_list = data_configs.get('ae_files')\n",
    "    ae_file = os.path.join(data_configs.get('dir'), ae_list[4])\n",
    "    x_adv = np.load(ae_file)\n",
    "\n",
    "    # evaluate the undefended model on the AE\n",
    "    print(\">>> Evaluating UM on [{}], it may take a while...\".format(ae_file))\n",
    "    pred_adv_um = undefended.predict(x_adv)\n",
    "    err_um = error_rate(y_pred=pred_adv_um, y_true=labels, correct_on_bs=corrections)\n",
    "    # track the result\n",
    "    results['UM'] = err_um\n",
    "\n",
    "    # evaluate the ensemble on the AE\n",
    "    print(\">>> Evaluating ensemble on [{}], it may take a while...\".format(ae_file))\n",
    "    pred_adv_ens = ensemble.predict(x_adv)\n",
    "    err_ens = error_rate(y_pred=pred_adv_ens, y_true=labels, correct_on_bs=corrections)\n",
    "    # track the result\n",
    "    results['Ensemble'] = err_ens\n",
    "\n",
    "    # evaluate the baseline on the AE\n",
    "    print(\">>> Evaluating baseline model on [{}], it may take a while...\".format(ae_file))\n",
    "    pred_adv_bl = baseline.predict(x_adv)\n",
    "    err_bl = error_rate(y_pred=pred_adv_bl, y_true=labels, correct_on_bs=corrections)\n",
    "    # track the result\n",
    "    results['PGD-ADT'] = err_bl\n",
    "\n",
    "    print(\">>> Evaluations on [{}]:\\n{}\".format(ae_file, results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_from_json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-8d25dbeb1eab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# parse configurations (into a dictionary) from json file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrans_configs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_from_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../configs/demo/athena-mnist.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel_configs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_from_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../configs/demo/model-mnist.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdata_configs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_from_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../configs/demo/data-mnist.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_from_json' is not defined"
     ]
    }
   ],
   "source": [
    "# parse configurations (into a dictionary) from json file\n",
    "trans_configs = load_from_json('../configs/demo/athena-mnist.json')\n",
    "model_configs = load_from_json('../configs/demo/model-mnist.json')\n",
    "data_configs = load_from_json('../configs/demo/data-mnist.json')\n",
    "\n",
    "# -------- test transformations -------------\n",
    "evaluate(trans_configs=trans_configs,\n",
    "         model_configs=model_configs,\n",
    "         data_configs=data_configs,\n",
    "         save=False,\n",
    "         output_dir=args.output_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
